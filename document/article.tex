\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage{listings, color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\usepackage[scaled]{beramono}
\usepackage{courier, natbib}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref, adjustbox}
\usepackage[small,bf]{caption}
\usepackage{float}
\usepackage{booktabs}
\usepackage{algpseudocode, algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicuntil}{\textbf{Until:}}
\renewcommand{\algorithmicrepeat}{\textbf{Repeat:}}

\newcommand{\subf}[2]{%
  {\small\begin{tabular}[t]{@{}c@{}}
  #1\\#2
  \end{tabular}}%
}

\lstset{
  language=Python,
  backgroundcolor=\color{white},
  breaklines=true,
  captionpos=b,
  commentstyle=\color{mygreen},
  showstringspaces=false,
  formfeed=newpage,
  keepspaces=true,
  stringstyle=\color{mymauve},
  keywordstyle=\color{blue},
  numbers=left,
  tabsize=4,
  numberstyle=\footnotesize,
  basicstyle=\footnotesize,
  morekeywords={models, lambda, forms, True, False, self}
}

\newcommand{\code}[2] {
  \hrulefill
  \subsection*{#1}
  \lstinputlisting{#2}
  \vspace{2em}
}

\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}

\input defs.tex

\title{\texttt{CompleteThat}\\ A python package for  low-rank matrix completion\\EEOR E4650 Course Project
}
\author{Joshua Edgerton \and Esteban Fajardo}
\date{January 5, 2014}

\begin{document}

\maketitle

\begin{abstract}
We have developed a Python package (\texttt{CompleteThat}) to perform low rank matrix completion. Given a low rank matrix with partial entries we implemented different methods to solve the matrix completion problem:  First, for ``small'' problems we implemented two memory-based algorithms  following the work by Tanner and Wei~\cite{Tanner:2014} to find, given a number $r$, the matrix with rank $r$ that best fits the data using the Frobenius norm. Second, when the problem does not fit into memory, we implemented an out-of-core algorithm (stochastic gradient descent) following the approach in Zhang~\cite{zhang:2004} and Bottou~\cite{bottou:2012}. The package is available at \url{https://pypi.python.org/pypi/completethat/0.1dev}. 
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
Matrix factorization has recently seen a large growth in popularity within the mathematics, statistics, and computer science communities as industry continues to apply machine learning techniques on a wide array of problems and scenarios. For our convex optimization project, we decided to take some of the more interesting and applicable topics and algorithms of our class and develop a python package to implement them. We briefly discuss the theory behind matrix factorizing, the models and approaches we choose to use and the algorithms for the optimization. Later, we walk through two case studies illustrating the usefulness and applicability in the context of image processing and a recommendation system for Yahoo music and movie data. 

\section{Matrix Completion Problem}
The matrix completion problem can be formally stated as follows. We are interested in recovering a matrix $M \in \reals^{n_1 \times n_2}$ but only get to observe a number $m \ll n_1n_2$ of its entries. Thus, we want to find a solution to the following optimization problem

\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \rank(X)\\
    \mbox{subject to} & X_{ij} = M_{ij} \qquad (i,j)\in \Omega
    \end{array}
    \label{eq:original_problem1}
\end{equation}
where $X\in \reals^{n_1 \times n_2}$ is the decision variable and $\rank(X)$ is equal to the rank of the matrix $X$. The problem (\ref{eq:original_problem1}) seeks the simplest matrix fitting the observed data. Of course, if there were only one low-rank object fitting the data, this would recover $M$.  
In order to fix the notation correctly define the projector $P_{\Omega}:\reals^{n_1\times n_2} \rightarrow \reals^{n_1\times n_2} $ as

\begin{equation*}
P_{\Omega}(A) = \left \{ 
	\begin{array}{lr}
		A_{ij} & : (i,j) \in \Omega\\
		0 & : \mathrm{otherwise}
	\end{array}
	\right.
\end{equation*}\\
Using the projector, Problem (\ref{eq:original_problem1}) can be rewritten as
\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \rank(X)\\
    \mbox{subject to} & P_{\Omega}(X) = P_{\Omega}(M)
    \end{array}
    \label{eq:original_problem2}
\end{equation}

Unfortunately, this optimization problem has been shown to be NP-hard and all know algorithms which provide exact solutions require time doubly exponential in the dimension of the matrix both in theory and in practice~\cite{Candes:2009}. Also, the $\rank(X)$ makes the problem non-convex. 

Several approximations to the problem exists. One, the ``tightest convex relaxation of $\rank(X)$''~\cite{Fazel:2002} is the following problem

\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}   &  \|X\|_* :=\sum_{i=1}^r \sigma_i(X) \\
    \mbox{subject to} & P_{\Omega}(X) = P_{\Omega}(M)
    \end{array}
    \label{eq:nucl_problem}
\end{equation}

where $\sigma_i(X)$ denotes the $i$th largest singular value of $X$ and $\|X\|_*$ is the called the nuclear norm. The main point of this relaxation is that the nuclear norm is a convex function and thus can be optimized efficiently via semidefinite programming or by iterative soft thresholding algorithms~\cite{cai:2010}~\cite{goldfarb:2011}.

Alternative to nuclear norm minimization there have been many algorithms which are designed to target the following optimization problem
\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}_{Y,Z}   & \frac{1}{2} \|P_{\Omega}(YZ) - P_{\Omega}(M)\|^2_F \\
    \end{array}
    \label{eq:frob_problem}
\end{equation}
where $X=YZ$ is the completed matrix and $Y\in \reals^{n_1\times r}, Z\in \reals^{r\times n_2}$ and $r$ represents the (hopefully small) rank of the matrix $X$. The main point of this relaxation is the use of an alternating minimization scheme or Gauss-Seidel or 2-block coordinate descent method, where first $Y$ is fixed and then the problem is reduced to a convex, standard least squares problem on $Z$. Later, $Z$ is fixed and then the problem is reduced to a convex, standard least squares problem on $Y$.

\section{Implementation}
\texttt{CompleteThat} is a python package that solves the low rank matrix completion problem. Given a low rank matrix with partial entries the package solves an optimization problem to estimate the missing entries. We allow the user to choose between several optimization algorithms including two in memory based algorithms, alternating steepest descent and scaled alternating steepest descent, and one out of memory fitting procedure using stochastic gradient descent. We use extensively the numerical libraries \texttt{spicy} and \texttt{numpy}, and the current implementation includes two classes, \texttt{MatrixCompletion} for in memory based algorithms and \texttt{MatrixCompletionBD} for the memory fitting procedure. The package is available worldwide and can be
downloaded from \url{https://pypi.python.org/pypi/completethat/0.1dev}.

\subsection*{CVX}
Problem (\ref{eq:nucl_problem}) can be easily solved directly using CVX, a package for specifying and solving convex programs~\cite{cvx}, ~\cite{gb08}, with the following code:

\begin{verbatim}
index = find(~isnan(M));
cvx_begin
    variable X(size(M));
    minimize norm_nuc(X)
    % s.t.
    X(index) == M(index);
cvx_end
\end{verbatim}

Where M is a MATLAB matrix with nan on the missing entries and \texttt{X(index) == M(index)} corresponds to the $P_{\Omega}(X) = P_{\Omega}(M)$ restriction. However, the computation is very slow and for any matrix greater than 100x100 the computational time is too high. Since this is clearly unsatisfactory for practical purposes, specific algorithms were implemented on the package. 

\subsection*{Algorithms}
\subsubsection*{Low rank matrix completion by alternating steepest descent methods}
The  alternating steepest descent methods implemented on \texttt{CompleteThat} solve the Problem (\ref{eq:frob_problem}) above.  Let the objective function of the optimization problem $f: \reals^{n_1\times r}\times \reals^{r\times n_2} \rightarrow \reals$ be defined as
\begin{equation}
	f(Y,Z):=\frac{1}{2} \|P_{\Omega}(YZ) - P_{\Omega}(M)\|^2_F.
	\label{eq:ASD_prob}
\end{equation}

To solve efficiently the above problem, Tanner and Wei~\cite{Tanner:2014} use a single step of simple line-search along the gradient descent directions.  Let us write $f(Y, Z)$ as $f_Z(Y)$ when $Z$ is held constant and $f_Y(Z)$ when $Y$ is held constant and let $t_y, t_z$ be the steepest descent step sizes for descent directions.

The Alternating Steepest Descent Method (ASD) applies steepest gradient descent to $f(Y, Z)$ in (\ref{eq:ASD_prob}) alternatively with respect to $Y$ and $Z$. It can be shown that the directions of gradient ascent and the steepest descent step sizes are~\cite{Tanner:2014} :
\[
\nabla f_{Z}(Y) = -(P_{\Omega}(M) - P_{\Omega}(Y Z))Z^T\
\]
\[
\nabla f_{Y}(Z) = -Y^T(P_{\Omega}(M)-P_{\Omega}(YZ)).
\]
\[
t_{y} = \frac{\|  \nabla f_{Z}(Y) \|^2_F}{\| P_{\Omega}(\nabla f_{Z} (Y)Z\|^2_F}
\]
\[
 t_{z} = \frac{\|  \nabla f_{Y}(Z) \|^2_F}{\| P_{\Omega}(Y \nabla f_Y (Z)) \|^2_F}
\]

\begin{algorithm}[h]
\caption{Alternating Steepest Descent (ASD)~\cite{Tanner:2014}}
\begin{algorithmic}
\Require $r \in \reals, P_{\Omega}(M), Y_0\in \reals^{n_1\times r}, Z_0\in \reals^{r\times n_2}$
\Repeat
	\State $\nabla f_{Z_i}(Y_i) = -(P_{\Omega}(M) - P_{\Omega}(Y_i Z_i))Z_i^T, t_{y_i} = \frac{\|  \nabla f_{Z_i}(Y_i) \|^2_F}{\| P_{\Omega}(\nabla f_{Z_i} (Y_i)Z_i \|^2_F}$
	\State $Y_{i+1} = Y_i - t_{y_i} f_{Z_i}(X_i)$
	\State $\nabla f_{Y_{i+1}}(Z_i) = -Y^T_{i+1}(P_{\Omega}(M)-P_{\Omega}(Y_{i+1}Z_i)), t_{z_i} = \frac{\|  \nabla f_{Y_{i+1}}(Z_i) \|^2_F}{\| P_{\Omega}(Y_{i+1} \nabla f_Y{i+1} (Z_i)) \|^2_F}$
	\State $Z_{i+1} = Z_i - t_{z_i}\nabla f_{Y_{i+1}}(Z_i)$
	\State $i = i +1$
\Until {termination criteria is reached}
\Ensure $X:= Y_i Z_i \in \reals^{n_1\times n_2}$
\end{algorithmic}
\end{algorithm}

The Scaled Alternating Steepest Descent Method (sASD) is an accelerated version of ASD and uses a scaled gradient descent direction with exact line-search. Its main motivation for the scaled factors is the explicit expression for the Newton directions for the problem (\ref{eq:ASD_prob}) if all the entries in the matrix $M$ were know:
\[
(M - YZ)Z^T(ZZ^T)^{-1}
\]
\[
(X^TX)^{-1}X^T(M-YZ)
\]
which are the gradient descent directions scaled by $(ZZ^T)^{-1}$ and $(X^TX)^{-1}$.

\begin{algorithm}[h]
\caption{Scaled Alternating Steepest Descent (sASD)~\cite{Tanner:2014}}
\begin{algorithmic}
\Require $r \in \reals, P_{\Omega}(M), Y_0\in \reals^{n_1\times r}, Z_0\in \reals^{r\times n_2}$
\Repeat
	\State $\nabla f_{Z_i}(Y_i) = -(P_{\Omega}(M) - P_{\Omega}(Y_i Z_i))Z_i^T$
	\State $d_{y_i} = -\nabla f_{Z_i} (Y_i)(Z_iZ_i^T)^{-1}, t_{y_i} =\frac{ -\nabla f_{Z_i}(Y_i)\cdot d_{y_i}}{\| P_{\Omega} (d_{y_i}Z_i)\|_F^2}$
	\State $Y_{i+1} = Y_i + t_{y_i}d_{y_i}$
	\State $\nabla f_{Y_{i+1}} (Z_i) = - Y_{i+1}^T(P_{\Omega}(M) - P_{\Omega}(Y_{i+1}Z_i))$
	\State $d_{z_i} = (Y_{i+1}^TY_{i+1}^{-1} \nabla f_{Y_{i+1}}(Z_i), t_{z_i} = \frac{-\nabla f_{Y_{i+1}}(Z_i)\cdot d_{z_i}}{\| P_{\Omega}(Y_{i+1}d_{z_i})\|_F^2}$
	\State $Z_{i+1} = Z_i + t_{z_i} d_{z_i}$
	\State $i = i + 1$
\Until {termination criteria is reached}
\Ensure $X:= Y_i Z_i \in \reals^{n_1\times n_2}$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Low rank matrix completion by stochastic gradient descent}
The formal problem as defined in the model section is stated in the standard language of linear algebra and the typical optimizations using gradient methods require the calculation of the gradient based on the sum of all the error terms, meaning that when updating our latent feature vectors, we calculate the gradient based on all the data in our ratings file. Given the scale of data that is common in today's digital world, the need for more efficient and memory friendly algorithms arises as calculating the gradient on a large enough file will be computationally expensive and time consuming. 

Let $U$ be the set of users and $I$ the set of items. Let $K\subset U \times I$ be the set of known ratings and $r_{ij}$ the rating that user $i$ gave to item $j, \forall (i,j)\in K$. Let $p_i$  be the latent feature vector for user $i$ of length equal to hypothesized rank. Let $q_j$ be the latent feature vector for item $j$ of length equal to hypothesized rank. The stochastic gradient descent method solves the following optimization problem:
\begin{equation}
    \begin{array}{ll}
    \mbox{minimize}_{r_{ij}\in K}   & f:= (r_{ij} - \hat r_{ij})^2 = e_{ij}^2\\
    \mbox{subject to} & r_{ij} = p_i^T q_j
    \end{array}
    \label{eq:SGD}
\end{equation}

With Stochastic gradient descent, a variation of the standard batch gradient optimization, instead of calculating the gradient using all of the ratings data, we approximate the gradient of our objective function using just the current record and update just the latent feature vectors of the user and item pertaining to the current rating. Thus one of the highlights of our package is that we simply read the file in line by line, running the algorithm in place. After each update, we simply read in the next line of the file and repeat.\footnote{We note that our formulation is not convex but, due to shuffling of the data and the way the gradients are estimated, the algorithm will converge to at least a local minimum.}

Our RAM requirements for the algorithm decrease significantly with this sort of out-of-memory implementation. For example, with the Yahoo Music Data we only needed to store the parameters for each user and item, rather than the entire 115 million records of our file plus the parameters. This results in storing ~2 million parameters in a python dictionary (a sort of built in hash function and table) rather than 115 million records in memory.  Thus we are essentially exchanging the luxury of using less memory and computational resources for more time and iterations spent with stochastic gradient descent.

After the algorithm converges or the maximum number of iterations are me have a set of user and item latent vectors which can be used to compute the estimated rating $r_{ij}$ by simply taking the dot product of the two vectors. Similarly the entire ratings matrix could easily be built with the vectors although we have neglected to do so for practicality reasons. 

\begin{algorithm}[H]
\caption{Stochastic Gradient Descent (SGD)}
\begin{algorithmic}
\Require $r_{ij} \in \reals,  \forall (i,j)\in K$
\State Initialize Parameters: $\forall i\in U$ randomly initialize $p_i\in \reals^r$. $\forall j\in I$ randomly initialize  $q_j\in \reals^r$
\Repeat
	\State Read rating $r_{ij}$
	\State Compute error $e_{ij} = r_{ij} - p_i^Tq_j$
	\State Compute gradients: 
	\State $\qquad \qquad \frac{\partial f}{\partial p_i} = -2e_{ij}q_j$
	\State $\qquad \qquad \frac{\partial f}{\partial q_j} = -2e_{ij}p_i$
	\State Update parameters:
	\State $\qquad \qquad p_i:=p_i + 2\alpha e_{ij} q_j$
	\State $\qquad \qquad q_j:=q_j + 2\alpha e_{ij} p_i$
	\State Update MSE: $\text{MSE} := \text{MSE} + e_{ij}$
\Until {termination criteria is reached}
\State Shuffle Data Set
\Ensure $p_i~~\forall i\in U; q_j ~~\forall j\in I$
\end{algorithmic}
\end{algorithm}

\paragraph{Notes}
\begin{itemize}
            
	\item Step Size: The algorithms performance is highly dependent on the step size ($\alpha$) parameter of the update equations. Too small of a alpha will result in convergence taking a very long time and lower the chance of finding the global objective. Too large of a alpha will result in convergence not happening as the the algorithm continually 'overshoots' the optimal value. Thus an essential part of using this algorithm will be experimentation with the step size. Currently, the step size (alpha parameter) is fully adjustable, with the default being a .01 decreasing 30\% each pass over the data until it reaches 1e-6 whereby it will remain constant until the stopping criterion of the algorithm are reached.
	
	\item Shuffle: When dealing with large data files we needed to be clever with how we shuffle our data. Stochastic gradient descent performs much better with shuffling of the data after each pass over the file thus shuffling is essential; yet shuffling is no trivial task for files too big to fit entirely into the computers random access memory. We explored various shuffling methods for both in-memory and out-of-memory but ultimately decided with using a batch pseudo-shuffle. Our batch shuffle method shuffles the file in chunks. So we read in some large portion (the batch size) of the file that will fit in memory, we shuffle the ratings of that batch using built in python functions, and lastly we write the shuffled ratings back to a text file, and continue sequentially until the all of the file has been shuffled. Following this method we say 'pseudo-shuffled' since the file is not truly shuffled. It is shuffled within the batch/portion of the file it belongs to but still not truly shuffled within the entire file. The larger the batches, the better the shuffled ratings, until of course we read the entire file into memory and shuffle it, the ideal situation. Hopefully by choosing a large enough batch size for reading in, our file will be shuffled well enough. We also explored a more robust method of shuffling the data by additionally shuffling the shuffled batches whereas before we wrote the shuffled ratings back to a file sequentially.  However ultimately the noticeably bigger time complexity of moving and appending large files together via Linux did not prove worthwhile given almost negligible returns in convergence

\end{itemize}

\subsection*{Code Example}
For matrices that fit into memory use the \texttt{MatrixCompletion} module. Otherwise, use the \texttt{MatrixCompletionBD} module. 

\subsubsection*{MatrixCompletion}
Given a \texttt{numpy} matrix $M$ with \texttt{numpy.nan} on the missing entries, the matrix completion problem can be solved (using ASD, for example) as:
\begin{verbatim}
>>> from completethat import MatrixCompletion
>>> problem = MatrixCompletion(M)
>>> problem.complete_it("ASD")
>>> X = problem.get_matrix() #Desired matrix
>>> out_info = problem.get_out() #Extra information
\end{verbatim}

\subsubsection*{MatrixCompletionBD}
Given a csv file with the input data with three fields (user id, item id , rating) the matrix completion problem can be solved as:

\begin{verbatim}
>>> from completethat import MatrixCompletionBD
>>> problem = MatrixCompletionBD('input_data.txt')
>>> problem.train_sgd(dimension=6,init_step_size=.01,min_step=.000001, 
       reltol=.001,rand_init_scale=10, maxiter=1000,
       batch_size_sgd=50000,shuffle=True)
>>> problem.validate_sgd('test_data.txt')
>>> problem.save_model()
\end{verbatim}

\section{Case Studies}
\subsection*{Yahoo movies and music reviews database}
The Yahoo music and movie data sets are publicly available datasets published by Yahoo for use to the public for recreational and research purposes. The music training data set is a small 4 mb tab-delimited file consisting of roughly 210,000 user-movie ratings. The test set has roughly 10,000 records. There are actually two ratings schemas available, one on a 5-point scale and the other on a 13-point scale and we arbitrarily chose to use the 5-point scale. The Yahoo music data is a larger 2.2 gb file of 115 million user-song ratings. The ratings are on a 100 point scale.
 
We look to evaluate the performance of our algorithms on these two datasets, comparing the various algorithms implemented versus a naive benchmark estimate where we used the mean of the training set as our estimator for all records in the test set. Then we compared how well the algorithms performed versus the benchmark as well as Mahout.  Apache Mahout is a mainstream scalable and fast machine learning library implemented in java which has a recommendation engine component similar to MatrixCompletionBD but uses alternating least squares for the optimization.  Thus given the origins and popularity of Mahout, we are also interested in comparing it to our python package?s performance.
The naïve estimator for the yahoo music data is 4.088. This is the mean value of the ratings in the training set so we will compute the MSE on the test set using the benchmark, which yields an RMSE of 1.10 (Note we use RMSE here only because Mahout uses RMSE). A quick run of Mahout yields 1.17 and CompleteThat's SGD model yielded a 1.19 (we used rank 7 but larger ranks will outperform the naïve baseline as we show in our table). So our model yielded a pretty similar RMSE as the MAHOUT package. Note that for both of these instances we used a factorization of rank equal to seven and the CompleteThat stochastic gradient descent algorithm ran slightly faster than Mahout's, with our SGD algorithm running on 220,000 ratings and converging in less than a minute (.537 minutes versus 1.4 minutes)!

The Yahoo Music data is a lot larger and provided a much more interesting scenario but unfortunately due to the time constraints and how long it took to converge on the Yahoo Music data, we didn?t get as much testing done here. It does take a very long time for the stochastic gradient descent algorithm to converge, but does run. In experimentation with features of rank six, we saw it converging in approximately 9 hours, with results roughly on par relative to the naïve baseline using the training set. Unfortunately, we weren?t able to get Apache Mahout working on our local machines with that scale of data, thus have no real comparison to be made in terms of how well our package does scale, but if nothing else we can say that our algorithm is fairly invariant to the size of the data.

Table~\ref{tab:yahoo_data} shows the performance of the SGD on the Yahoo music with varying the rank of the user and item latent features. We see that the algorithm performs best on the test set using a rank around 15, and it is evident that too large a rank is detrimental to performance.  
\begin{table}[h]
\centering
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{@{}ccccc@{}}
\toprule
\multicolumn{1}{l}{\textbf{Step size}} & \multicolumn{1}{l}{\textbf{Iterations}} & \multicolumn{1}{l}{\textbf{Minutes}} & \multicolumn{1}{l}{\textbf{Train MSE}} & \multicolumn{1}{l}{\textbf{Test RMSE}} \\ \midrule
1 & 8 & 0.585 & 1.411 & 1.757 \\
2 & 8 & 0.598 & 1.304 & 1.550 \\
3 & 8 & 0.628 & 1.235 & 1.483 \\
4 & 8 & 0.617 & 1.180 & 1.397 \\
5 & 7 & 0.538 & 1.136 & 1.351 \\
6 & 7 & 0.534 & 1.100 & 1.244 \\
7 & 7 & 0.534 & 1.071 & 1.194 \\
8 & 7 & 0.541 & 1.046 & 1.127 \\
9 & 7 & 0.537 & 1.027 & 1.128 \\
10 & 7 & 0.529 & 1.010 & 1.102 \\
15 & 7 & 0.551 & 0.971 & 1.063 \\
20 & 7 & 0.540 & 0.976 & 1.075 \\
30 & 8 & 0.614 & 1.045 & 1.359 \\ \bottomrule
\end{tabular}
}
\caption{RMSE vs Step Size vs Rank}
\label{tab:yahoo_data}
\end{table}

\clearpage
\subsection*{Columbia University photographs}
In this section, we demonstrate the applicability of the package to image processing problems. Grayscale pictures can be transform into a rectangular matrix where each element of the matrix determines the intensity of the corresponding pixel. For convenience, most of the current digital files use integer numbers between 0 and 255. By randomly erasing, or setting to zero, a predefined proportion of the pixels of an image the problem of completing the image can be cast as a matrix completion problem, where the missing entries are the those with zero on it. 

For illustration we used three  512 x 512 grayscale photographs taken by one of the authors of the Columbia University campus on December, 2014. The original, erased and reconstructed images are shown on Figure~\ref{fig:columbia_pics}, where we erased 35\% of the pixels of each image.  In addition, to illustrate the ease of use of the package, we also present the (very short) script used to recover the photographs:

\lstinputlisting[numbers=none]{columbia_images.py}

\begin{figure}[H]
\resizebox{\textwidth}{!}{%
\makebox[\textwidth][c]
{
\begin{tabular}{ccc}
\includegraphics[width=65mm]{figures/columbia1.png}
&
\includegraphics[width=65mm]{figures/columbia2.png}

&
\includegraphics[width=65mm]{figures/columbia3.png}

\\

\includegraphics[width=65mm]{figures/columbia4.png}
&
\includegraphics[width=65mm]{figures/columbia5.png}
&
\includegraphics[width=65mm]{figures/columbia6.png}
\\

\includegraphics[width=65mm]{figures/columbia7.png}
&
\includegraphics[width=65mm]{figures/columbia8.png}
&
\includegraphics[width=65mm]{figures/columbia9.png}
\\
\end{tabular}
}
}

\caption{ASD method applied three different Columbia University photographs using random sampling}
\label{fig:columbia_pics}
\end{figure}

\section{Future Work}

We have plans for various improvements as we continue working with the \texttt{CompleteThat} python package. Overall, we would like to test the software and integrate automated testing cases into the software developing cycle. Also, documentation for the package and its functions, including examples, is high on our priority list. In addition, we hope to incorporate more rigorous error checking and make   available more customization to the user. 

For the memory-based algorithms it is well know that noise in the data introduces overfitting on the estimated matrix. Following the approach taken by Mazumder et al.~\cite{mazumder:2010}  where they solve the same objetive function as the problems above (using the Frobenious norm) but introducing the nuclear norm as regularizer to account for overfitting, we would like to introduce a regularization option into the matrix completion procedure. 

For the stochastic gradient descent method, we plan to add a lambda penalty feature for combatting overfitting as well as considering options for more advanced versions of our basic latent factor model as demonstrated by Koren~\cite{koren:2008} in his paper on the prize-winning Netflix models. 
For all algorithms, we would like to explore and research various hardware and software optimization techniques for faster code. 

Finally, given the similarities between matrix completion and robust principal component analysis one further extension to the functionality of the package could be implementing the robust-PCA procedure outlined by Cand\'es et al.~\cite{candes:2011}.

\newpage
\bibliographystyle{alpha}
\bibliography{references}

\newpage
\section{Source code}
On the whole, the package directory structure looks like Figure~\ref{fig:code_scheme}. In what follows we present the source code of the package.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{./figures/code_scheme.pdf}
    \caption{CompleteThat structure}
    \label{fig:code_scheme}
\end{figure}

\code{completethat/matrix\_completion.py}{../code/completethat/matrix_completion.py}
\code{completethat/matrix\_\_init\_\_.py}{../code/completethat/__init__.py}
\code{setup.py}{../code/setup.py}
\hrulefill
\subsection*{README.rst}
\lstinputlisting{../code/README.rst}

\end{document}